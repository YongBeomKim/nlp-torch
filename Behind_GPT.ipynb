{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toTl_eUFWYQj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Behind the Scenes of ChatGPT\n",
    "---\n",
    "[Pico GPT](https://github.com/jaymody/picoGPT/blob/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785/gpt2_pico.py#L43-L58). 코드를 단계적으로 이해해 보겠습니다. [Original Colab Code](https://colab.research.google.com/drive/1yEA-lkxJm5eSO6LkcIvkdWz_bE_q2LQj?usp=sharing)\n",
    "- Author: [Jay Mody](https://jaykmody.com)\n",
    "- Code: [picoGPT](https://github.com/jaymody/picoGPT)\n",
    "- Blog Post: [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNfTHhz-lb7n"
   },
   "source": [
    "# Input / Output\n",
    "---\n",
    "가장 간단한 자기회귀 (autoregressive) 모델은 다음과 같습니다.\n",
    "\n",
    "```python\n",
    "for _ in range(n_tokens_to_generate):\n",
    "    next_word = gpt(prompt)\n",
    "    prompt = prompt + next_word\n",
    "```\n",
    "\n",
    "자기회귀 모델에서 사용된 `gpt` 함수는 다음과 같은 형태를 갖고 있습니다.\n",
    "\n",
    "```python\n",
    "def gpt(prompt: str) -> str:\n",
    "    next_word = # beep boop neural network magic\n",
    "    return next_word\n",
    "```\n",
    "\n",
    "하지만 신경망 내부 에서는 숫자로 임베딩한 값을 사용하기 때문에, 다음과 같이 변경 했습니다.\n",
    "\n",
    "```python\n",
    "def gpt(inputs: list[int]) -> list[list[float]]:\n",
    "    # inputs has shape [n_seq]\n",
    "    # output has shape [n_seq, n_vocab]\n",
    "    output = # beep boop neural network magic\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sCXWHeLNFx9"
   },
   "source": [
    "# 1 Input\n",
    "## 1-1 Encoding\n",
    "- One Hot Encoding : Encoding & Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzlYIR9oHaBT",
    "outputId": "35876eac-3541-4ec6-abee-af866dd8fae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n",
      "all\n",
      "heroes\n",
      "wear\n",
      "capes\n"
     ]
    }
   ],
   "source": [
    "# 정수 값으로 Token 을 임베딩 하는 예제 입니다.\n",
    "inputs = [1,     0,    2,      4,     6]\n",
    "vocab  = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "for id in inputs:\n",
    "    print(vocab[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "81AydCBcOGPb",
    "outputId": "de440e8e-6663-4337-e734-adbefe448a0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 2, 4], 'not all heroes wear')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코더와 디코더를 클래스로 작성하였습니다\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.token_to_id = {token: i for i, token in enumerate(vocab)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.split(\" \")\n",
    "        ids = [self.token_to_id[token] for token in tokens]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.id_to_token[id] for id in ids]\n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "\n",
    "# 문장을 Token 으로 분리 후, 숫자로 Embadding 하는 클래스\n",
    "# .encode() : converts a str -> list[int]\n",
    "# .decode() : back a list[int] -> str\n",
    "tokenizer = WhitespaceTokenizer(vocab)\n",
    "ids = tokenizer.encode(\"not all heroes wear\")\n",
    "text = tokenizer.decode(ids)\n",
    "ids, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "81AydCBcOGPb",
    "outputId": "de440e8e-6663-4337-e734-adbefe448a0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not', 'all', 'heroes', 'wear']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 맨 앞에서 장의한 `vocab` 을 활용하여, 문장의 Token 을 Mapping 합니다.\n",
    "tokens = [vocab[i] for i in ids]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sCXWHeLNFx9"
   },
   "source": [
    "## 1-2 Input Errors\n",
    "오류가 발생하는 상황들을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "EzeTX-LaIk3d",
    "outputId": "fcf0e9ef-37f5-4b08-db34-28a7caf15a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError : 'shoes'\n",
      "KeyError : ''\n",
      "KeyError : ''\n",
      "KeyError : 'capes!'\n"
     ]
    }
   ],
   "source": [
    "def check_input_error(texts:str):\n",
    "    try:\n",
    "        print(tokenizer.encode(texts))\n",
    "    except KeyError as E:\n",
    "        print(f\"KeyError : {E}\")\n",
    "\n",
    "# Q1 `vocab` 에 없는 단어를 입력하면 어떻게 될까?\n",
    "check_input_error(\"not all heroes wear shoes\")\n",
    "# Q2 `vocab` 에 없는 multiple spaces 가 있으면 어떻게 될까?\n",
    "check_input_error(\"not all heroes    wear\")\n",
    "# Q3 '' 을 입력하면 어떻게 될 까?\n",
    "check_input_error(\"\")\n",
    "# Q4 `punctuation` 기호가 포함되어 있으면 어떻게 될 까?\n",
    "check_input_error(\"not all heroes wear capes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Input 내용을 마치며\n",
    "이처럼 공백을 기준으로 Token 을 나누는 방법 보다는 [Byte-Pair Encoding](https://huggingface.co/course/chapter6/5?fw=pt) 또는 [WordPiece](https://huggingface.co/course/chapter6/6?fw=pt) 와 같은 고급화 기법을 주로 활용 합니다. 고급화 기법 이라도 동작 원리는 앞에서 살펴본 내용과 동일 합니다.\n",
    "1. 문자열을 입력 받습니다.\n",
    "2. **<span style=\"color:orange\">토큰화 도구</span>** 를 사용하여 문자열을 토큰이라는 작은 조각으로 분해합니다.\n",
    "3. 토큰을 정수로 매핑하기 위해, **<span style=\"color:orange\">어휘 사전</span>** 을 사용합니다.\n",
    "\n",
    "그렇다면 **<span style=\"color:orange\">어휘 사전</span>** 에 무엇이 들어갈지 어떻게 결정할까요? **Tokenizer** 에서는 여러 **<span style=\"color:orange\">최적의 어휘가 무엇인지 알아내는 '훈련' 프로세스</span>** 도 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scs2yYlTO3gI"
   },
   "source": [
    "<br/>\n",
    "\n",
    "# 2 OutPut\n",
    "## 2-1 Output\n",
    "출력 형태는 2차원의 배열 (`output[i][j]`) 형태를 갖습니다. `vocab[j]` 값의 의미는 바로 다음에 연결되는 `inputs[i+1]` Token 을 예측한 확률 입니다.\n",
    "```python\n",
    "output = gpt(inputs)\n",
    "#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "# output[0] =  [0.75    0.1     0.0       0.15    0.0   0.0    0.0  ]\n",
    "# \"not\" 보단 \"all\" 의 가능성을 더 높게 예측\n",
    "\n",
    "#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "# output[1] =  [0.0     0.0      0.8     0.1    0.0    0.0   0.1  ]\n",
    "# [\"not\", \"all\"] 시퀀스 부분을 입력할 때, 모델은 \"heroes\" 를 가장 높게 예측\n",
    "\n",
    "#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "# output[-1] = [0.0     0.0     0.0     0.1     0.0    0.05  0.85  ]\n",
    "# 전체 시퀀스인 [\"not\", \"all\", \"heroes\", \"wear\"] 을 입력하면, \"capes\" 확률이 가장 높음\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B-tN4Yb1QSzx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 'capes')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab  = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "inputs = [1, 0, 2, 4] # \"not\" \"all\" \"heroes\" \"wear\"\n",
    "\n",
    "import numpy as np\n",
    "output = np.array([\n",
    "    [0.75, 0.1, 0.0, 0.15, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.8, 0.1, 0.0, 0.0, 0.1],\n",
    "    # ...\n",
    "    [0.0, 0.0, 0.0, 0.1, 0.0, 0.05, 0.85],\n",
    "])\n",
    "next_id = np.argmax(output[-1]) # 마지막 배열에서 가장 높은 값의 인덱스\n",
    "next_id, vocab[next_id]         # 해당 인덱스에서 임베딩하는 Token 값을 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XQLvhaMEUk_b"
   },
   "source": [
    "## 2-2 Output 내용을 마치며\n",
    "이처럼 **<span style=\"color:orange\">해당 시점에서 가장 높은 확률을 선택</span>** 하는 방법을 **greedy decoding** 또는 **greedy sampling** 이라고 합니다.\n",
    "\n",
    "이 방법은 1등과 2등이 미묘한 차이를 갖는 경우라면, 2등이 정답일 가능성도 고려해야 합니다. 이와같은 상황에 적용할 수 있도록 **분포** 값을 적용하는 [Beam Search](https://blog.naver.com/PostView.naver?blogId=sooftware&logNo=221809101199) 또는 [top-p and top-k](https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p) 등의 방법이 있습니다.\n",
    "\n",
    "이번 과정에서는 빠르게 진행 가능한 **greedy sampling** 을 활용해 보겠습니다.\n",
    "\n",
    "앞에서 살펴본 자기회귀 (autoregressive) 모델에서 `np.argmax()` 를 추가 합니다.\n",
    "\n",
    "```python\n",
    "for _ in range(n_tokens_to_generate):\n",
    "    output = gpt(ids)\n",
    "    next_id = np.argmax(output[-1])\n",
    "    ids.append(next_id)\n",
    "```\n",
    "\n",
    "이처럼 GPT 에서는 바로 이어지는 다음 단어만 예측하면 되는데, 여러개의 확률 벡터를 입력받는 이유는 왜일까요? 마지막 추론 과정에서는 `Output` 챕터에서 살펴본 것처럼 마지막 확률값(`output[-1]`) 을 활용하지만 전체적인 훈련 과정은 모든 출력값을(`outputs[i]` 값은 `ids[i+1]` 인덱스 값) 활용 합니다. \n",
    "\n",
    "이렇게 하면 각각의 Token 마다 **Forward Passes** 를 수행하지 않고, 모든 Token 에 대해서 **병렬로 계산** 을 할 수 있게되어 GPT 훈련을 매우 효율적으로 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# 3 GPT 구현하기\n",
    "- Load Tokenizer, Hyperparamters, and Parameters\n",
    "## 3-1 Tokenizer\n",
    "[BPE tokenizer](https://huggingface.co/course/chapter6/5?fw=pt) 은 GPT2 에서 사용되고 있습니다.\n",
    "```python\n",
    "! git clone https://github.com/jaymody/picoGPT\n",
    "import sys\n",
    "sys.path.append(\"picoGPT\")\n",
    "from gpt2 import main\n",
    "output = main(\n",
    "    prompt=\"Alan Turing theorized that computers would one day become\",\n",
    "    model_size=\"124M\",\n",
    "    n_tokens_to_generate=8\n",
    ")\n",
    "output # ' the most powerful machines on the planet.'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3673, 477, 10281, 5806, 1451, 274, 13], 'Not all heroes wear capes.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tensorflow as tf\n",
    "sys.path.append(\"./gits/picoGPT\")\n",
    "\n",
    "from encoder import Encoder\n",
    "from utils import download_gpt2_files, load_gpt2_params_from_tf_ckpt\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
    "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)\n",
    "\n",
    "def load_encoder_hparams_and_params(model_size, models_dir):\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "    # download files if necessary\n",
    "    if not tf_ckpt_path:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        download_gpt2_files(model_size, model_dir)\n",
    "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "    encoder = get_encoder(model_size, models_dir)\n",
    "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params  = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
    "    return encoder, hparams, params\n",
    "\n",
    "# from utils import load_encoder_hparams_and_params\n",
    "tokenizer, hparams, params = load_encoder_hparams_and_params(\n",
    "    model_size=\"124M\",\n",
    "    models_dir=\"data\"\n",
    ")\n",
    "ids = tokenizer.encode(\"Not all heroes wear capes.\")\n",
    "ids, tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS6mZRQnYkhb"
   },
   "source": [
    "Using the vocabulary of the tokenizer (stored in `encoder.decoder`), we can take a peek at what the actual tokens look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_tZmlQ7YVPr",
    "outputId": "896826b2-1b4c-4bb3-dbd7-fbb68330cf56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not', 'Ġall', 'Ġheroes', 'Ġwear', 'Ġcap', 'es', '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decoder[i] for i in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2KEPTxcYqG-"
   },
   "source": [
    "Notice, sometimes our tokens are words (e.g. `Not`), sometimes they are words but with a space in front of them (e.g. `Ġall`, the [`Ġ` represents a space](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/bpe.py#L22-L33)), sometimes there are part of a word (e.g. capes is split into `Ġcap` and `es`), and sometimes they are punctuation (e.g. `.`).\n",
    "\n",
    "One nice thing about BPE is that it can encode any arbitrary string. If it encounters something that is not present in the vocabulary, it just breaks it down into substrings it does understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CL0F-xTvmCw1",
    "outputId": "76537959-bed5-4f6e-bc42-8db08744f27c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['z', 'j', 'q', 'fl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decoder[i] for i in tokenizer.encode(\"zjqfl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvPtiUb8Y4Ld",
    "outputId": "c1348f2a-7cd6-4c0d-c35e-b37d5bd47e1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz1tLzZYoq-W"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "`n_vocab`: Number of tokens in our vocabulary\n",
    "\n",
    "`n_layer`: Number of layers (determines the \"depth\" of the network)\n",
    "\n",
    "`n_embd`: Embedding dimension (determines the \"width\" of the network)\n",
    "\n",
    "`n_ctx`: Maximum possible sequence length of the input\n",
    "\n",
    "`n_head`: Number of attention heads (`n_embd` must be divisible by `n_head`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsHKFA1fotOf",
    "outputId": "2821341d-8f69-4043-c276-5da01c5e21ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ldMBVVVpPf8"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rY5P_i5Mlmm",
    "outputId": "5201cc03-a98f-4d24-8212-136be5d662e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'ln_f', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YoBuUXOZ_XW",
    "outputId": "4dedbeff-df17-46b6-8da2-d09fc62bdcf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11010301, -0.03926672,  0.03310751, ..., -0.1363697 ,\n",
       "         0.01506208,  0.04531523],\n",
       "       [ 0.04034033, -0.04861503,  0.04624869, ...,  0.08605453,\n",
       "         0.00253983,  0.04318958],\n",
       "       [-0.12746179,  0.04793796,  0.18410145, ...,  0.08991534,\n",
       "        -0.12972379, -0.08785918],\n",
       "       ...,\n",
       "       [-0.04453601, -0.05483596,  0.01225674, ...,  0.10435229,\n",
       "         0.09783269, -0.06952604],\n",
       "       [ 0.1860082 ,  0.01665728,  0.04611587, ..., -0.09625227,\n",
       "         0.07847701, -0.02245961],\n",
       "       [ 0.05135201, -0.02768905,  0.0499369 , ...,  0.00704835,\n",
       "         0.15519823,  0.12067825]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"wte\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK_liHAFp3XZ",
    "outputId": "d0dc3889-7cd7-49b2-e12c-bcac6ddeafe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def param_count(d):\n",
    "    if isinstance(d, np.ndarray):\n",
    "        return np.size(d)\n",
    "    elif isinstance(d, list):\n",
    "        return sum(param_count(v) for v in d)\n",
    "    elif isinstance(d, dict):\n",
    "        return sum(param_count(v) for v in d.values())\n",
    "    else:\n",
    "        ValueError(\"uh oh\")\n",
    "\n",
    "param_count(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URutrNlZQH-d"
   },
   "source": [
    "## GPT Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg5pvkrFNUon"
   },
   "source": [
    "Import the `gpt2` function from picoGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhY6PozgNTfu"
   },
   "outputs": [],
   "source": [
    "from gpt2 import gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01KA-UO-QM-4"
   },
   "source": [
    "Let's inspect the function signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cR0SZYcbNcrt",
    "outputId": "97bd4edf-f85e-4f49-9754-51e8f859d72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gpt2 in module gpt2:\n",
      "\n",
      "gpt2(inputs, wte, wpe, blocks, ln_f, n_head)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzFX7B6NNi33"
   },
   "source": [
    "Notice that the function signature input includes some extra stuff in addition to `inputs`:\n",
    "\n",
    "* `wte`, `wpe`, `blocks`, and `ln_f` the parameters of our model.\n",
    "* `n_head` is a hyperparameter that is needed during the forward pass.\n",
    "\n",
    "So, we can call our gpt2 function as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkCtxNnsQYk_",
    "outputId": "26f07250-1e7d-4bb5-a76b-ec2bc01a959f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 17180, 1595, 470, 2121, 1290, 422, 262]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"The apple doesn't fall far from the\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1Du2NHSQi4h",
    "outputId": "ca1ea1cb-b0bc-4f07-a9aa-f70432252c67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 50257)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = gpt2(input_ids, **params, n_head=hparams[\"n_head\"])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHoK5HqpQoFy",
    "outputId": "d637627c-63f7-4bcc-e61c-52242b17d583"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5509"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_id = np.argmax(output[-1])\n",
    "next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RyZbjVsNQpe2",
    "outputId": "08c3ab9e-039a-458c-baa4-24dd8d5f0692"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Ġtree'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder[next_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j-jzhNNamZC"
   },
   "source": [
    "## Final Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJXEgaHeREcA"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmWQn3tPqfsQ"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from gpt2 import gpt2\n",
    "from utils import load_encoder_hparams_and_params\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    # auto-regressive decode loop\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):\n",
    "        # model forward pass\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)\n",
    "\n",
    "        # greedy sampling\n",
    "        next_id = np.argmax(logits[-1])\n",
    "\n",
    "        # append prediction to input\n",
    "        inputs.append(int(next_id))\n",
    "\n",
    "    # only return generated ids\n",
    "    output_ids = inputs[len(inputs) - n_tokens_to_generate :]\n",
    "    return output_ids\n",
    "\n",
    "\n",
    "def main(prompt: str, n_tokens_to_generate: int = 40, model_size: str = \"124M\", models_dir: str = \"models\"):\n",
    "    # load encoder, hparams, and params from the released open-ai gpt-2 files\n",
    "    tokenizer, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
    "\n",
    "    # encode the input string using the BPE tokenizer\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "    # make sure we are not surpassing the max sequence length of our model\n",
    "    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "\n",
    "    # generate output ids\n",
    "    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "    # decode the ids back into a string\n",
    "    output_text = tokenizer.decode(output_ids)\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "0mSmxa4pM3CL",
    "outputId": "5e165833-2f7c-4f8b-e0e5-061789ad0384"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|██████████| 8/8 [00:05<00:00,  1.60it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' the most powerful machines on the planet.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(\n",
    "    \"Alan Turing theorized that computers would one day become\",\n",
    "    n_tokens_to_generate=8,\n",
    "    model_size=\"124M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BexoDaOGRkvv"
   },
   "source": [
    "# Next Steps?\n",
    "---\n",
    "See the full [blog post](https://jaykmody.com/blog/gpt-from-scratch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyce-L35RtJ2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
